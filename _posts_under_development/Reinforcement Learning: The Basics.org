Hey! This is my first post, and it is meant to be an extremely simple introduction to
reinforcement learning. Let's dive in.

In reinforcement learning, usually we have an agent that wants to learn an optimal set of
actions (called a *policy*, denoted by $\pi$). It can sense the whole environment (the
environment is fully observable), but it doesn't know how the environment works, neither the
results of the actions. Also, we assume that each action may result in many different outcomes,
given an unknown probabilistic mapping from actions to outcomes (called the *transition model*:
$P(s^{'} | s,a)$). Also, for each state the agent receives a reward, a mapping called the
*reward function*: $R(s)$. All of the above means that the agent faces a /Markov Decision
Process/ (MDP).

To solve this problem, we consider three types of agent design:

| Reflex agent              | Q-learning (model free)             | Model based                   |
|---------------------------+-------------------------------------+-------------------------------|
| <>                        | <>                                  | <>                            |
| Searches the policy space | Learns an action â†’ utility function | Learns the *transition model* |
|                           |                                     | and the *reward function*     |
