@article{Schapire2013,
abstract = {Boosting is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate rules. The AdaBoost algorithm of Freund and Schapire was the first practical boosting algorithm, and remains one of the most widely used and studied, with applications in numerous fields. This chapter aims to review some of the many perspectives and analyses of AdaBoost that have been applied to explain or understand it as a learning method, with comparisons of both the strengths and weaknesses of the various approaches.},
author = {Schapire, Robert E.},
doi = {10.1007/978-3-642-41136-6_5},
file = {:home/arthurcgusmao/Documents/Articles/explaining-adaboost.pdf:pdf},
isbn = {9783642411366},
journal = {Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik},
pages = {37--52},
title = {{Explaining adaboost}},
year = {2013}
}
@article{Schapire1989,
author = {Schapire, Robert E.},
file = {:home/arthurcgusmao/Documents/Articles/schapire1990.pdf:pdf},
keywords = {learnability theory,learning from examples,machine learning,pac learning,polynomial-time},
pages = {197--227},
title = {{The Strength of Weak Learnability}},
url = {http://www.dtic.mil/dtic/tr/fulltext/u2/a216389.pdf},
volume = {227},
year = {1989}
}
@article{Ehlen2007,
author = {Ehlen, Stephan},
doi = {10.4467/20838476SI.14.001.3018},
file = {:home/arthurcgusmao/Documents/Articles/IntroToBoosting.pdf:pdf},
isbn = {9780761973034},
number = {4},
pages = {1--5},
title = {{a Short Introduction To boosting}},
volume = {1},
year = {2007}
}
@article{Henrique,
author = {Henrique, Francisco and Vieira, Otte and Cozman, Fabio Gagliardi},
file = {:home/arthurcgusmao/GD/Professional/Academic Articles/francisco{\_}sum.pdf:pdf},
keywords = {ing of bayesian networks,probabilistic logic programming,score-based structure learn-},
title = {{Learning Probabilistic Logic Programs by Score Maximization: Tractable Cases}}
}
@book{Russell2010,
abstract = {The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Russell, Stuart J. and Norvig, Peter},
booktitle = {Artificial Intelligence},
doi = {10.1017/S0269888900007724},
eprint = {arXiv:1011.1669v3},
file = {:home/arthurcgusmao/GD/Professional/Artificial Intelligence/Artificial Intelligence - A Modern Approach (Norvig {\&} Russell, 2010)/Artificial Intelligence - A Modern Approach (2010).pdf:pdf},
isbn = {0137903952},
issn = {00206539},
keywords = {Artificial intelligence,Programming,ai,ai learning machine textbook,artificial-intelligence,artificial-intelligence knowledge-representation m,juergen,local search,planning,scheduling,search space},
pages = {1132},
pmid = {20949757},
title = {{Artificial Intelligence: A Modern Approach}},
url = {http://amazon.de/o/ASIN/0130803022/},
year = {2010}
}
@article{Yang2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.08367v1},
author = {Yang, Fan and Yang, Zhilin and Cohen, William W},
eprint = {arXiv:1702.08367v1},
file = {:home/arthurcgusmao/Dropbox/Arthur-Eduardo/Differentiable Learning of Logical Rules for Knowledge Base Completion.pdf:pdf},
title = {{Differentiable Learning of Logical Rules for Knowledge Base Completion}},
year = {2016}
}
@article{Bareinboim2016,
abstract = {The exponential growth of electronically accessible information has led some to conjecture that data alone can replace sub- stantive knowledge in practical decision making and scientific explorations. In this paper, we argue that traditional scientific methodologies that have been successful in the natural and bio- medical sciences would still be necessary for big data applications, albeit tasked with new challenges: to go beyond predictions and, using information from multiple sources, provide users with rea- soned recommendations for actions and policies. The feasibility of meeting these challenges is demonstrated here using specific data fusion tasks, following a brief introduction to the structural causal model (SCM) framework (1â€“3).},
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
file = {:home/arthurcgusmao/GD/Professional/Academic Articles/PNAS-2016-Bareinboim-7345-52.pdf:pdf},
isbn = {10916490 (Electronic)},
issn = {0027-8424},
journal = {Pnas},
number = {27},
pages = {7345--7352},
pmid = {27382148},
title = {{Causal inference and the data-fusion problem}},
url = {www.pnas.org/cgi/doi/10.1073/pnas.1510507113},
volume = {113},
year = {2016}
}
@unpublished{DeRaedt2015,
abstract = {We study the problem of inducing logic programs in a probabilistic setting, in which both the example descriptions and their classification can be proba-bilistic. The setting is incorporated in the proba-bilistic rule learner ProbFOIL + , which combines principles of the rule learner FOIL with ProbLog, a probabilistic Prolog. We illustrate the approach by applying it to the knowledge base of NELL, the Never-Ending Language Learner.},
author = {{De Raedt}, Luc and Dries, Anton and Thon, Ingo and {Van Den Broeck}, Guy and Verbeke, Mathias},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {:home/arthurcgusmao/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Raedt et al. - 2015 - Inducing probabilistic relational rules from probabilistic examples.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {1835--1843},
title = {{Inducing probabilistic relational rules from probabilistic examples}},
volume = {2015-Janua},
year = {2015}
}
@article{Cohen2016,
abstract = {Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.},
archivePrefix = {arXiv},
arxivId = {1605.06523},
author = {Cohen, William W.},
eprint = {1605.06523},
file = {:home/arthurcgusmao/Dropbox/Arthur-Eduardo/TensorLog.pdf:pdf},
number = {Nips},
title = {{TensorLog: A Differentiable Deductive Database}},
url = {http://arxiv.org/abs/1605.06523},
year = {2016}
}
@misc{ wiki:AdaBoost,
  author = "Wikipedia",
  title = "AdaBoost --- Wikipedia{,} The Free Encyclopedia",
  year = "2017",
  url = "https://en.wikipedia.org/w/index.php?title=AdaBoost&oldid=768996379",
  note = "[Online; accessed 16-May-2017]"
}
                  @online{youtube:MIT,
                          title = {17. Learning: Boosting},
                          date = {2014},
                          organization = {Youtube},
                          author = {MIT OpenCourseWare},
                          url = {https://www.youtube.com/watch?v=UHBmv7qCey4},
                      }
           @article{Long:2010:RCN:1713649.1713653,
                   author = {Long, Philip M. and Servedio, Rocco A.},
                   title = {Random Classification Noise Defeats All Convex Potential Boosters},
                   journal = {Mach. Learn.},
                   issue_date = {March     2010},
                   volume = {78},
                   number = {3},
                   month = mar,
                   year = {2010},
                   issn = {0885-6125},
                   pages = {287--304},
                   numpages = {18},
                   url = {http://dx.doi.org/10.1007/s10994-009-5165-z},
                   doi = {10.1007/s10994-009-5165-z},
                   acmid = {1713653},
                   publisher = {Kluwer Academic Publishers},
                   address = {Hingham, MA, USA},
                   keywords = {Boosting, Convex loss, Learning theory, Misclassification noise, Noise-tolerant learning, Potential boosting},
                  }
                  @misc{ wiki:boosting,
                     author = "Wikipedia",
                     title = "Boosting (machine learning) --- Wikipedia{,} The Free Encyclopedia",
                     year = "2017",
                     url = "https://en.wikipedia.org/w/index.php?title=Boosting_(machine_learning)&oldid=778023582",
                     note = "[Online; accessed 16-May-2017]"
                   }